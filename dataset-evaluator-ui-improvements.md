# Dataset Evaluator UI Clarity Improvements

## Problem

Users were confused when setting up evaluators that target dataset runs, assuming the evaluator would run immediately on their dataset after creation. However, dataset evaluators are only triggered when dataset experiments are executed, not when the evaluator is created.

## Changes Made

### 1. Experiment Creation Form (`CreateExperimentsForm.tsx`)

**Before:** "Will run against your experiment results."
**After:** "These evaluators will automatically run when this experiment is executed, scoring all traces generated by the experiment."

**Impact:** Makes it explicit that evaluators run during experiment execution.

### 2. Time Scope Description (`eval-form-descriptions.tsx`)

**Added:** Clear note for dataset evaluators explaining they are triggered by experiment execution, not by evaluator creation.

**New text:** "Note: Dataset evaluators are triggered when dataset experiments are executed, not when the evaluator is created."

### 3. Evaluator Form Time Scope Section (`inner-evaluator-form.tsx`)

**Before:**

- "New dataset run items"
- "Existing dataset run items"

**After:**

- "Future experiments" with description: "Triggered when new dataset experiments are executed"
- "Historical experiments" with description: "Run on all existing experiment data immediately"

**Impact:** Much clearer about what "new" and "existing" mean in the context of dataset evaluators.

### 4. Target Data Section (`inner-evaluator-form.tsx`)

**Before:** Tab labeled "Experiment runs" with no description
**After:**

- Tab labeled "Dataset experiments"
- Added description: "This evaluator will be triggered when dataset experiments are executed. It will not run immediately upon creation."

### 5. Template Selector (`template-selector.tsx`)

**Added:** Tooltip on the evaluator selection button: "Configure evaluators that will automatically run when dataset experiments are executed"

**Impact:** Provides immediate context when users hover over the evaluator selector.

## Summary

These changes address the core confusion by:

1. **Explicit timing clarification** - Multiple places now clearly state that dataset evaluators run during experiment execution
2. **Better labeling** - "Future experiments" vs "dataset run items" is much clearer
3. **Contextual help** - Tooltips and descriptions provide just-in-time clarification
4. **Consistent messaging** - All UI elements now consistently explain the trigger mechanism

The improvements maintain the existing functionality while significantly reducing user confusion about when evaluators actually execute.
